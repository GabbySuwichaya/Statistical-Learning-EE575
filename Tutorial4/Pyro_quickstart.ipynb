{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick start on Pyro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial 4: Bayesian Neural network (Pyro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to support our learning in Lecture 10. Bayesian neural network \n",
    "\n",
    "by *Suwichaya Suwanwimolkul, Ph.D.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/GabbySuwichaya/Statistical-Learning-EE575/blob/master/Tutorial4/Pyro_quickstart.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will build a BNN with Pyro ...\n",
    "\n",
    "Each network has 2 linear layers with 1 activating function (sigmoid).\n",
    "We set the same learning rate and number of epoches to compare which model is better... \n",
    "\n",
    "You can skip to each of the following topics...\n",
    "- [Install package](#installing-package)\n",
    "- [Import data](#import-data) \n",
    "- [Bayesian neural network (Pyro)](#bayesian-neural-network-Pyro)    \n",
    "- [Training BNN](#training-bnn) \n",
    "- [Prediction BNN](#prediction)\n",
    "\n",
    "Our implementation is based on [Pyro Stochastic Variational Inference](https://pyro.ai/examples/bayesian_regression.html#Bayesian-Regression-with-Pyro%E2%80%99s-Stochastic-Variational-Inference-(SVI)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip3 install torch torchvision torchaudio\n",
    "#! pip install pyro-ppl "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will learn about this example through the car-price dataset. \n",
    "Just import the dataset from the MS team! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch import nn \n",
    "import math\n",
    "\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X_train_, y_train_, X_test_=None, y_test_=None, mode=\"Train\"):\n",
    "        self.training_data  = X_train_\n",
    "        self.training_label = y_train_\n",
    "        \n",
    "        if mode == \"Train\":\n",
    "            self.data  = X_train_\n",
    "            self.label = y_train_\n",
    "            self.length = X_train_.shape[0]\n",
    "\n",
    "        elif  mode == \"Test\" or mode == \"Valid\":\n",
    "            assert X_test_ is not None\n",
    "            assert y_test_ is not None\n",
    "\n",
    "            self.data   = X_test_\n",
    "            self.label  = y_test_\n",
    "            self.length = X_test_.shape[0]\n",
    " \n",
    "\n",
    "        self.XScaler    = StandardScaler()\n",
    "        self.training_data = self.XScaler.fit_transform(self.training_data).astype(\"float32\")\n",
    "        self.data          = self.XScaler.transform(self.data).astype(\"float32\") \n",
    "\n",
    "        self.YScaler = StandardScaler()\n",
    "        self.training_label = self.YScaler.fit_transform(self.training_label).astype(\"float32\") \n",
    "        self.label          = self.YScaler.transform(self.label).astype(\"float32\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx, :] \n",
    "        label  = self.label[idx] \n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldername = \"vehicle-dataset\"\n",
    "\n",
    "with zipfile.ZipFile(\"vehicle-dataset-from-cardekho.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(foldername)\n",
    "\n",
    "df = pd.read_csv('%s/car data.csv' % foldername)\n",
    "\n",
    "df[['Car_Name', 'Fuel_Type','Seller_Type','Transmission']] = df.loc[:,['Car_Name', 'Fuel_Type','Seller_Type','Transmission']].apply(LabelEncoder().fit_transform)\n",
    "\n",
    "\n",
    "Feat_List = ['Car_Name', 'Year', 'Present_Price', 'Kms_Driven', 'Fuel_Type','Seller_Type','Transmission', 'Owner']\n",
    "dfX = df.loc[:, Feat_List].values\n",
    "dfy = df['Selling_Price'].values.reshape(-1,1)\n",
    "\n",
    "X_train_, X_test_, y_train_, y_test_ = train_test_split(dfX, dfy, test_size=0.2, random_state= 0)\n",
    "X_train_, X_val_, y_train_, y_val_   = train_test_split(X_train_, y_train_, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "TrainingData = CustomDataset(X_train_, y_train_, mode=\"Train\")\n",
    "ValidateData = CustomDataset(X_train_, y_train_, X_val_,  y_val_,  mode=\"Valid\")\n",
    "TestingData  = CustomDataset(X_train_, y_train_, X_test_, y_test_, mode=\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### You can use the following cell to explore your training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[60] ...my message : 100%|██████████| 61/61 [00:00<00:00, 4104.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "testing_dataloader = DataLoader(TestingData, batch_size=1, shuffle=False)\n",
    "\n",
    "pbar  = tqdm(testing_dataloader)\n",
    "\n",
    "for i, data in enumerate(pbar): \n",
    "    \n",
    "    data_x, data_y = data  \n",
    "    \n",
    "    pbar.set_description(\"[%d] ...my message \" % (i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Neural Network (Pyro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's assume that the weights and biases are sampled from Gaussian distribution ==> `AutoDiagonalNormal` \n",
    "\n",
    "- Then, the configuration for BNN follows: \n",
    "\n",
    "    - Putting the new layers `PyroModule[nn.Linear]` in `BayesianLinear`. Be careful these layers are different from the one from torch.  \n",
    "    - Also note that we still need the data sampling parts to simulate the noise in $\\hat{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.nn import PyroModule, PyroParam, PyroSample\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import math\n",
    "import pyro\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "in_features  = 8\n",
    "out_features = 1\n",
    "\n",
    "class BayesianLinear(PyroModule):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__() \n",
    "        self.linear = PyroModule[nn.Sequential](\n",
    "                PyroModule[nn.Linear](in_features, 16), \n",
    "                PyroModule[nn.Linear](16, out_features),\n",
    "                PyroModule[nn.Sigmoid](), \n",
    "            )\n",
    "        \n",
    "        for i,m in enumerate(self.linear.modules()):\n",
    "            for  name, value in list(m.named_parameters(recurse=False)):\n",
    "                setattr(m, name, PyroSample(prior=dist.Normal(0, 1).expand(value.shape).to_event(value.dim())))\n",
    "                 \n",
    "    def forward(self, x, y=None):\n",
    "        sigma = pyro.sample(\"sigma\", dist.Uniform(0., 10.))\n",
    "        mean = self.linear(x).squeeze(-1)\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            obs = pyro.sample(\"obs\", dist.Normal(mean, sigma), obs=y)\n",
    "        return mean\n",
    "    \n",
    "model = BayesianLinear(in_features=8, out_features=1)\n",
    "guide = AutoDiagonalNormal(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"748pt\" height=\"155pt\"\n",
       " viewBox=\"0.00 0.00 747.94 155.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 151)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-151 743.94,-151 743.94,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_data</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"305.35,-8 305.35,-83 375.35,-83 375.35,-8 305.35,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"351.35\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"14.00\">data</text>\n",
       "</g>\n",
       "<!-- sigma -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>sigma</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"38.35\" cy=\"-129\" rx=\"38.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"38.35\" y=\"-125.3\" font-family=\"Times,serif\" font-size=\"14.00\">sigma</text>\n",
       "</g>\n",
       "<!-- obs -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>obs</title>\n",
       "<ellipse fill=\"gray\" stroke=\"black\" cx=\"340.35\" cy=\"-57\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"340.35\" y=\"-53.3\" font-family=\"Times,serif\" font-size=\"14.00\">obs</text>\n",
       "</g>\n",
       "<!-- sigma&#45;&gt;obs -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>sigma&#45;&gt;obs</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M66.97,-116.92C72.99,-114.79 79.34,-112.7 85.35,-111 162.32,-89.2 254.69,-72.22 304.41,-63.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"305.07,-67.25 314.35,-62.14 303.91,-60.34 305.07,-67.25\"/>\n",
       "</g>\n",
       "<!-- linear.0.weight -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>linear.0.weight</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"174.35\" cy=\"-129\" rx=\"79.89\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"174.35\" y=\"-125.3\" font-family=\"Times,serif\" font-size=\"14.00\">linear.0.weight</text>\n",
       "</g>\n",
       "<!-- linear.0.weight&#45;&gt;obs -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>linear.0.weight&#45;&gt;obs</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M210.37,-112.81C239.71,-100.44 280.73,-83.14 308.8,-71.3\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"310.29,-74.47 318.14,-67.36 307.57,-68.02 310.29,-74.47\"/>\n",
       "</g>\n",
       "<!-- linear.0.bias -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>linear.0.bias</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"340.35\" cy=\"-129\" rx=\"67.69\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"340.35\" y=\"-125.3\" font-family=\"Times,serif\" font-size=\"14.00\">linear.0.bias</text>\n",
       "</g>\n",
       "<!-- linear.0.bias&#45;&gt;obs -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>linear.0.bias&#45;&gt;obs</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M340.35,-110.7C340.35,-102.98 340.35,-93.71 340.35,-85.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"343.85,-85.1 340.35,-75.1 336.85,-85.1 343.85,-85.1\"/>\n",
       "</g>\n",
       "<!-- linear.1.weight -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>linear.1.weight</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"506.35\" cy=\"-129\" rx=\"79.89\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"506.35\" y=\"-125.3\" font-family=\"Times,serif\" font-size=\"14.00\">linear.1.weight</text>\n",
       "</g>\n",
       "<!-- linear.1.weight&#45;&gt;obs -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>linear.1.weight&#45;&gt;obs</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M470.33,-112.81C440.98,-100.44 399.96,-83.14 371.9,-71.3\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"373.13,-68.02 362.55,-67.36 370.41,-74.47 373.13,-68.02\"/>\n",
       "</g>\n",
       "<!-- linear.1.bias -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>linear.1.bias</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"672.35\" cy=\"-129\" rx=\"67.69\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"672.35\" y=\"-125.3\" font-family=\"Times,serif\" font-size=\"14.00\">linear.1.bias</text>\n",
       "</g>\n",
       "<!-- linear.1.bias&#45;&gt;obs -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>linear.1.bias&#45;&gt;obs</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M622.25,-116.85C613.29,-114.88 604.05,-112.86 595.35,-111 517.13,-94.24 425.23,-75.33 375.92,-65.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"376.57,-61.81 366.07,-63.24 375.16,-68.67 376.57,-61.81\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fa9d0166c90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x = torch.ones(1,8)\n",
    "data_y = torch.ones(1,1)\n",
    "pyro.render_model(model, model_args=(data_x,data_y,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training BNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back-by-propagation follows the steps in this figure:\n",
    "\n",
    "  <img src=\"figures/Fig2.png\" alt=\"BNN Training\" width=\"600\"/>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 59/192 [00:00<00:00, 198.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0001] loss: 289.7368\n",
      "[iteration 0011] loss: 26.6240\n",
      "[iteration 0021] loss: 13.0009\n",
      "[iteration 0031] loss: 8.4705\n",
      "[iteration 0041] loss: 5.8348\n",
      "[iteration 0051] loss: 4.0292\n",
      "[iteration 0061] loss: 3.5018\n",
      "[iteration 0071] loss: 2.5705\n",
      "[iteration 0081] loss: 2.3233\n",
      "[iteration 0091] loss: 2.0039\n",
      "[iteration 0101] loss: 1.5912\n",
      "[iteration 0111] loss: 1.4251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [00:00<00:00, 317.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0121] loss: 0.9234\n",
      "[iteration 0131] loss: 0.9232\n",
      "[iteration 0141] loss: 0.7476\n",
      "[iteration 0151] loss: 0.6177\n",
      "[iteration 0161] loss: 0.6019\n",
      "[iteration 0171] loss: 0.4173\n",
      "[iteration 0181] loss: 0.3891\n",
      "[iteration 0191] loss: 0.2514\n",
      "[iteration 0001] loss: 68.0827\n",
      "[iteration 0011] loss: 4.0375\n",
      "[iteration 0021] loss: 2.5404\n",
      "[iteration 0031] loss: 1.3953\n",
      "[iteration 0041] loss: 0.6810\n",
      "[iteration 0051] loss: 0.5580\n",
      "[iteration 0061] loss: 0.2409\n",
      "[iteration 0071] loss: 0.2968\n",
      "[iteration 0081] loss: 0.1653\n",
      "[iteration 0091] loss: 0.1797\n",
      "[iteration 0101] loss: 0.1159\n",
      "[iteration 0111] loss: 0.0785\n",
      "[iteration 0121] loss: 0.0698\n",
      "[iteration 0131] loss: 0.0727\n",
      "[iteration 0141] loss: 0.0881\n",
      "[iteration 0151] loss: 0.0334\n",
      "[iteration 0161] loss: 0.0313\n",
      "[iteration 0171] loss: 0.0152\n",
      "[iteration 0181] loss: 0.0380\n",
      "[iteration 0191] loss: 0.0170\n",
      "[iteration 0001] loss: 5.6880\n",
      "[iteration 0011] loss: 0.3059\n",
      "[iteration 0021] loss: 0.1649\n",
      "[iteration 0031] loss: 0.1724\n",
      "[iteration 0041] loss: 0.1376\n",
      "[iteration 0051] loss: 0.5667\n",
      "[iteration 0061] loss: 0.0789\n",
      "[iteration 0071] loss: 0.0438\n",
      "[iteration 0081] loss: 0.0174\n",
      "[iteration 0091] loss: 0.0544\n",
      "[iteration 0101] loss: 0.0496\n",
      "[iteration 0111] loss: 0.0482\n",
      "[iteration 0121] loss: 0.0294\n",
      "[iteration 0131] loss: 0.0376\n",
      "[iteration 0141] loss: 0.0294\n",
      "[iteration 0151] loss: 0.0259\n",
      "[iteration 0161] loss: 0.0168\n",
      "[iteration 0171] loss: 0.0134\n",
      "[iteration 0181] loss: 0.0587\n",
      "[iteration 0191] loss: 0.0222\n",
      "[iteration 0001] loss: 4.3014\n",
      "[iteration 0011] loss: 0.2214\n",
      "[iteration 0021] loss: 0.2263\n",
      "[iteration 0031] loss: 0.1435\n",
      "[iteration 0041] loss: 0.1568\n",
      "[iteration 0051] loss: 0.0606\n",
      "[iteration 0061] loss: 0.0650\n",
      "[iteration 0071] loss: 0.0438\n",
      "[iteration 0081] loss: 0.0456\n",
      "[iteration 0091] loss: 0.0505\n",
      "[iteration 0101] loss: 0.0246\n",
      "[iteration 0111] loss: 0.0302\n",
      "[iteration 0121] loss: 0.0269\n",
      "[iteration 0131] loss: 0.0047\n",
      "[iteration 0141] loss: 0.0345\n",
      "[iteration 0151] loss: 0.0291\n",
      "[iteration 0161] loss: 0.0273\n",
      "[iteration 0171] loss: 0.0336\n",
      "[iteration 0181] loss: 0.0154\n",
      "[iteration 0191] loss: 0.0161\n",
      "[iteration 0001] loss: 2.9181\n",
      "[iteration 0011] loss: 0.3132\n",
      "[iteration 0021] loss: 0.1198\n",
      "[iteration 0031] loss: 0.1187\n",
      "[iteration 0041] loss: 0.1020\n",
      "[iteration 0051] loss: 0.0628\n",
      "[iteration 0061] loss: 0.0634\n",
      "[iteration 0071] loss: 0.0701\n",
      "[iteration 0081] loss: 0.0234\n",
      "[iteration 0091] loss: 0.0512\n",
      "[iteration 0101] loss: 0.0359\n",
      "[iteration 0111] loss: 0.0372\n",
      "[iteration 0121] loss: 0.0502\n",
      "[iteration 0131] loss: 0.0243\n",
      "[iteration 0141] loss: 0.0073\n",
      "[iteration 0151] loss: 0.0110\n",
      "[iteration 0161] loss: 0.0213\n",
      "[iteration 0171] loss: 0.0118\n",
      "[iteration 0181] loss: 0.0196\n",
      "[iteration 0191] loss: 0.0107\n",
      "[iteration 0001] loss: 1.4681\n",
      "[iteration 0011] loss: 0.3674\n",
      "[iteration 0021] loss: 0.0627\n",
      "[iteration 0031] loss: 0.2224\n",
      "[iteration 0041] loss: 0.1129\n",
      "[iteration 0051] loss: 0.0350\n",
      "[iteration 0061] loss: 0.0474\n",
      "[iteration 0071] loss: 0.0119\n",
      "[iteration 0081] loss: 0.0104\n",
      "[iteration 0091] loss: 0.0511\n",
      "[iteration 0101] loss: 0.0309\n",
      "[iteration 0111] loss: 0.0369\n",
      "[iteration 0121] loss: 0.0527\n",
      "[iteration 0131] loss: 0.0101\n",
      "[iteration 0141] loss: 0.0366\n",
      "[iteration 0151] loss: 0.0056\n",
      "[iteration 0161] loss: 0.0363\n",
      "[iteration 0171] loss: 0.0342\n",
      "[iteration 0181] loss: 0.0241\n",
      "[iteration 0191] loss: 0.0249\n",
      "[iteration 0001] loss: 3.8642\n",
      "[iteration 0011] loss: 0.3231\n",
      "[iteration 0021] loss: 0.1480\n",
      "[iteration 0031] loss: 0.1606\n",
      "[iteration 0041] loss: 0.1364\n",
      "[iteration 0051] loss: 0.0106\n",
      "[iteration 0061] loss: 0.0652\n",
      "[iteration 0071] loss: 0.0546\n",
      "[iteration 0081] loss: 0.0452\n",
      "[iteration 0091] loss: 0.0282\n",
      "[iteration 0101] loss: 0.0279\n",
      "[iteration 0111] loss: 0.0295\n",
      "[iteration 0121] loss: 0.0229\n",
      "[iteration 0131] loss: 0.0296\n",
      "[iteration 0141] loss: 0.0306\n",
      "[iteration 0151] loss: 0.0229\n",
      "[iteration 0161] loss: 0.0165\n",
      "[iteration 0171] loss: 0.0029\n",
      "[iteration 0181] loss: 0.0233\n",
      "[iteration 0191] loss: 0.0163\n",
      "[iteration 0001] loss: 2.7174\n",
      "[iteration 0011] loss: 0.3499\n",
      "[iteration 0021] loss: 0.2020\n",
      "[iteration 0031] loss: 0.0229\n",
      "[iteration 0041] loss: -0.0008\n",
      "[iteration 0051] loss: 0.0835\n",
      "[iteration 0061] loss: 0.0606\n",
      "[iteration 0071] loss: 0.0264\n",
      "[iteration 0081] loss: 0.0732\n",
      "[iteration 0091] loss: 0.0201\n",
      "[iteration 0101] loss: 0.0373\n",
      "[iteration 0111] loss: 0.0413\n",
      "[iteration 0121] loss: 0.0209\n",
      "[iteration 0131] loss: 0.0251\n",
      "[iteration 0141] loss: 0.0302\n",
      "[iteration 0151] loss: 0.0174\n",
      "[iteration 0161] loss: 0.0219\n",
      "[iteration 0171] loss: 0.0278\n",
      "[iteration 0181] loss: 0.0126\n",
      "[iteration 0191] loss: 0.0122\n",
      "[iteration 0001] loss: 2.1393\n",
      "[iteration 0011] loss: 0.2446\n",
      "[iteration 0021] loss: 0.2133\n",
      "[iteration 0031] loss: 0.1359\n",
      "[iteration 0041] loss: 0.1052\n",
      "[iteration 0051] loss: 0.0413\n",
      "[iteration 0061] loss: 0.0282\n",
      "[iteration 0071] loss: 0.0254\n",
      "[iteration 0081] loss: 0.0422\n",
      "[iteration 0091] loss: 0.0427\n",
      "[iteration 0101] loss: 0.0394\n",
      "[iteration 0111] loss: 0.0278\n",
      "[iteration 0121] loss: 0.0207\n",
      "[iteration 0131] loss: 0.0096\n",
      "[iteration 0141] loss: 0.0109\n",
      "[iteration 0151] loss: 0.0263\n",
      "[iteration 0161] loss: 0.0166\n",
      "[iteration 0171] loss: 0.0092\n",
      "[iteration 0181] loss: 0.0154\n",
      "[iteration 0191] loss: 0.0202\n",
      "[iteration 0001] loss: 2.7229\n",
      "[iteration 0011] loss: 0.2823\n",
      "[iteration 0021] loss: 0.1281\n",
      "[iteration 0031] loss: 0.1199\n",
      "[iteration 0041] loss: 0.0649\n",
      "[iteration 0051] loss: 0.0756\n",
      "[iteration 0061] loss: 0.0629\n",
      "[iteration 0071] loss: 0.0172\n",
      "[iteration 0081] loss: 0.0247\n",
      "[iteration 0091] loss: 0.0409\n",
      "[iteration 0101] loss: 0.0070\n",
      "[iteration 0111] loss: 0.0133\n",
      "[iteration 0121] loss: 0.0349\n",
      "[iteration 0131] loss: 0.0238\n",
      "[iteration 0141] loss: 0.0219\n",
      "[iteration 0151] loss: 0.0247\n",
      "[iteration 0161] loss: 0.0219\n",
      "[iteration 0171] loss: 0.0168\n",
      "[iteration 0181] loss: 0.0221\n",
      "[iteration 0191] loss: 0.0068\n",
      "[iteration 0001] loss: 3.6353\n",
      "[iteration 0011] loss: 0.3517\n",
      "[iteration 0021] loss: 0.2175\n",
      "[iteration 0031] loss: 0.2064\n",
      "[iteration 0041] loss: 0.0582\n",
      "[iteration 0051] loss: 0.0554\n",
      "[iteration 0061] loss: 0.0752\n",
      "[iteration 0071] loss: 0.0353\n",
      "[iteration 0081] loss: 0.0430\n",
      "[iteration 0091] loss: 0.0348\n",
      "[iteration 0101] loss: 0.0289\n",
      "[iteration 0111] loss: 0.0340\n",
      "[iteration 0121] loss: 0.0219\n",
      "[iteration 0131] loss: 0.0356\n",
      "[iteration 0141] loss: 0.0182\n",
      "[iteration 0151] loss: 0.0287\n",
      "[iteration 0161] loss: 0.0221\n",
      "[iteration 0171] loss: 0.0242\n",
      "[iteration 0181] loss: 0.0146\n",
      "[iteration 0191] loss: 0.0277\n",
      "[iteration 0001] loss: 3.6763\n",
      "[iteration 0011] loss: 0.2001\n",
      "[iteration 0021] loss: 0.1203\n",
      "[iteration 0031] loss: 0.1032\n",
      "[iteration 0041] loss: 0.0874\n",
      "[iteration 0051] loss: 0.0700\n",
      "[iteration 0061] loss: 0.0466\n",
      "[iteration 0071] loss: 0.0312\n",
      "[iteration 0081] loss: 0.0396\n",
      "[iteration 0091] loss: 0.0631\n",
      "[iteration 0101] loss: 0.0254\n",
      "[iteration 0111] loss: 0.0364\n",
      "[iteration 0121] loss: 0.0328\n",
      "[iteration 0131] loss: 0.0288\n",
      "[iteration 0141] loss: 0.0177\n",
      "[iteration 0151] loss: 0.0242\n",
      "[iteration 0161] loss: 0.0337\n",
      "[iteration 0171] loss: 0.0212\n",
      "[iteration 0181] loss: 0.0211\n",
      "[iteration 0191] loss: 0.0052\n",
      "[iteration 0001] loss: 2.9735\n",
      "[iteration 0011] loss: 0.4541\n",
      "[iteration 0021] loss: 0.3260\n",
      "[iteration 0031] loss: 0.1176\n",
      "[iteration 0041] loss: 0.0494\n",
      "[iteration 0051] loss: 0.0734\n",
      "[iteration 0061] loss: 0.0187\n",
      "[iteration 0071] loss: 0.0417\n",
      "[iteration 0081] loss: 0.0265\n",
      "[iteration 0091] loss: 0.0127\n",
      "[iteration 0101] loss: 0.0038\n",
      "[iteration 0111] loss: 0.0395\n",
      "[iteration 0121] loss: 0.0210\n",
      "[iteration 0131] loss: 0.0318\n",
      "[iteration 0141] loss: 0.0219\n",
      "[iteration 0151] loss: 0.0320\n",
      "[iteration 0161] loss: 0.0148\n",
      "[iteration 0171] loss: 0.0386\n",
      "[iteration 0181] loss: 0.0134\n",
      "[iteration 0191] loss: 0.0209\n",
      "[iteration 0001] loss: 2.7303\n",
      "[iteration 0011] loss: 0.3022\n",
      "[iteration 0021] loss: 0.1656\n",
      "[iteration 0031] loss: 0.0890\n",
      "[iteration 0041] loss: 0.0496\n",
      "[iteration 0051] loss: 0.0664\n",
      "[iteration 0061] loss: 0.0617\n",
      "[iteration 0071] loss: 0.0406\n",
      "[iteration 0081] loss: 0.0487\n",
      "[iteration 0091] loss: 0.0483\n",
      "[iteration 0101] loss: 0.0221\n",
      "[iteration 0111] loss: 0.0289\n",
      "[iteration 0121] loss: 0.0311\n",
      "[iteration 0131] loss: 0.0410\n",
      "[iteration 0141] loss: 0.0272\n",
      "[iteration 0151] loss: 0.0157\n",
      "[iteration 0161] loss: 0.0371\n",
      "[iteration 0171] loss: 0.0210\n",
      "[iteration 0181] loss: 0.0067\n",
      "[iteration 0191] loss: 0.0174\n",
      "[iteration 0001] loss: 2.0847\n",
      "[iteration 0011] loss: 0.0414\n",
      "[iteration 0021] loss: 0.2867\n",
      "[iteration 0031] loss: 0.0885\n",
      "[iteration 0041] loss: 0.1106\n",
      "[iteration 0051] loss: 0.0321\n",
      "[iteration 0061] loss: 0.0364\n",
      "[iteration 0071] loss: 0.0498\n",
      "[iteration 0081] loss: 0.0581\n",
      "[iteration 0091] loss: 0.0257\n",
      "[iteration 0101] loss: 0.0336\n",
      "[iteration 0111] loss: 0.0316\n",
      "[iteration 0121] loss: 0.0261\n",
      "[iteration 0131] loss: 0.0221\n",
      "[iteration 0141] loss: 0.0190\n",
      "[iteration 0151] loss: 0.0063\n",
      "[iteration 0161] loss: 0.0310\n",
      "[iteration 0171] loss: -0.0028\n",
      "[iteration 0181] loss: 0.0124\n",
      "[iteration 0191] loss: 0.0235\n",
      "[iteration 0001] loss: 1.1670\n",
      "[iteration 0011] loss: 0.4156\n",
      "[iteration 0021] loss: 0.1187\n",
      "[iteration 0031] loss: 0.1020\n",
      "[iteration 0041] loss: 0.0454\n",
      "[iteration 0051] loss: 0.0852\n",
      "[iteration 0061] loss: 0.0390\n",
      "[iteration 0071] loss: 0.0413\n",
      "[iteration 0081] loss: 0.0392\n",
      "[iteration 0091] loss: 0.0264\n",
      "[iteration 0101] loss: 0.0224\n",
      "[iteration 0111] loss: 0.0111\n",
      "[iteration 0121] loss: 0.0341\n",
      "[iteration 0131] loss: 0.0526\n",
      "[iteration 0141] loss: 0.0040\n",
      "[iteration 0151] loss: 0.0229\n",
      "[iteration 0161] loss: 0.0122\n",
      "[iteration 0171] loss: 0.0199\n",
      "[iteration 0181] loss: 0.0175\n",
      "[iteration 0191] loss: 0.0259\n",
      "[iteration 0001] loss: 3.2229\n",
      "[iteration 0011] loss: 0.1854\n",
      "[iteration 0021] loss: 0.1297\n",
      "[iteration 0031] loss: 0.1188\n",
      "[iteration 0041] loss: 0.0062\n",
      "[iteration 0051] loss: 0.0488\n",
      "[iteration 0061] loss: 0.0356\n",
      "[iteration 0071] loss: 0.0475\n",
      "[iteration 0081] loss: 0.0236\n",
      "[iteration 0091] loss: 0.0238\n",
      "[iteration 0101] loss: 0.0360\n",
      "[iteration 0111] loss: 0.0282\n",
      "[iteration 0121] loss: 0.0235\n",
      "[iteration 0131] loss: 0.0293\n",
      "[iteration 0141] loss: 0.0264\n",
      "[iteration 0151] loss: 0.0037\n",
      "[iteration 0161] loss: 0.0212\n",
      "[iteration 0171] loss: 0.0110\n",
      "[iteration 0181] loss: 0.0119\n",
      "[iteration 0191] loss: 0.0217\n",
      "[iteration 0001] loss: 5.8145\n",
      "[iteration 0011] loss: 0.3183\n",
      "[iteration 0021] loss: 0.1732\n",
      "[iteration 0031] loss: 0.1328\n",
      "[iteration 0041] loss: 0.0726\n",
      "[iteration 0051] loss: 0.0669\n",
      "[iteration 0061] loss: 0.0807\n",
      "[iteration 0071] loss: 0.0164\n",
      "[iteration 0081] loss: 0.0235\n",
      "[iteration 0091] loss: 0.0416\n",
      "[iteration 0101] loss: 0.0450\n",
      "[iteration 0111] loss: 0.0188\n",
      "[iteration 0121] loss: 0.0222\n",
      "[iteration 0131] loss: 0.0090\n",
      "[iteration 0141] loss: 0.0210\n",
      "[iteration 0151] loss: 0.0293\n",
      "[iteration 0161] loss: 0.0213\n",
      "[iteration 0171] loss: 0.0191\n",
      "[iteration 0181] loss: 0.0236\n",
      "[iteration 0191] loss: 0.0136\n",
      "[iteration 0001] loss: 2.6021\n",
      "[iteration 0011] loss: 0.4190\n",
      "[iteration 0021] loss: 0.1773\n",
      "[iteration 0031] loss: 0.1171\n",
      "[iteration 0041] loss: 0.0918\n",
      "[iteration 0051] loss: 0.0514\n",
      "[iteration 0061] loss: 0.0620\n",
      "[iteration 0071] loss: 0.0580\n",
      "[iteration 0081] loss: 0.0592\n",
      "[iteration 0091] loss: 0.0319\n",
      "[iteration 0101] loss: 0.0370\n",
      "[iteration 0111] loss: 0.0251\n",
      "[iteration 0121] loss: 0.0226\n",
      "[iteration 0131] loss: 0.0206\n",
      "[iteration 0141] loss: 0.0207\n",
      "[iteration 0151] loss: 0.0231\n",
      "[iteration 0161] loss: 0.0183\n",
      "[iteration 0171] loss: 0.0215\n",
      "[iteration 0181] loss: 0.0260\n",
      "[iteration 0191] loss: 0.0110\n",
      "[iteration 0001] loss: 5.0019\n",
      "[iteration 0011] loss: 0.1263\n",
      "[iteration 0021] loss: 0.1739\n",
      "[iteration 0031] loss: 0.1348\n",
      "[iteration 0041] loss: 0.1001\n",
      "[iteration 0051] loss: 0.0229\n",
      "[iteration 0061] loss: 0.0307\n",
      "[iteration 0071] loss: 0.0777\n",
      "[iteration 0081] loss: 0.0639\n",
      "[iteration 0091] loss: 0.0468\n",
      "[iteration 0101] loss: 0.0357\n",
      "[iteration 0111] loss: 0.0266\n",
      "[iteration 0121] loss: 0.0359\n",
      "[iteration 0131] loss: 0.0245\n",
      "[iteration 0141] loss: 0.0211\n",
      "[iteration 0151] loss: 0.0351\n",
      "[iteration 0161] loss: 0.0124\n",
      "[iteration 0171] loss: 0.0290\n",
      "[iteration 0181] loss: 0.0266\n",
      "[iteration 0191] loss: 0.0170\n"
     ]
    }
   ],
   "source": [
    "from pyro.infer import SVI, Trace_ELBO\n",
    "\n",
    "adam = pyro.optim.Adam({\"lr\": 0.01})\n",
    "svi = SVI(model, guide, adam, loss=Trace_ELBO())    \n",
    "\n",
    "training_dataloader = DataLoader(TrainingData, batch_size=1, shuffle=True)\n",
    "pbar  = tqdm(training_dataloader)\n",
    "\n",
    "\n",
    "for j in range(20):\n",
    "    # calculate the loss and take a gradient step\n",
    "    \n",
    "    for i, (x_data, y_data) in enumerate(pbar):\n",
    "\n",
    "        loss = svi.step(x_data, y_data)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (i + 1, loss / (i + 1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61/61 [00:02<00:00, 28.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== WLS =========================\n",
      "MAE: 4.339778166869364\n",
      "MSE: 29.52438914658042\n",
      "R2score: -647.956321190662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pyro.infer import Predictive \n",
    "\n",
    "predictive = Predictive(model, guide=guide, num_samples=100, return_sites=(\"obs\", \"_RETURN\"))\n",
    "import pdb\n",
    "testing_dataloader = DataLoader(TestingData, batch_size=1, shuffle=False)\n",
    "pbar  = tqdm(testing_dataloader)\n",
    "\n",
    "samples_list = []\n",
    "for i, (x_data, y_data) in enumerate(pbar):\n",
    "    samples = predictive(x_data)\n",
    "    av_output = samples[\"_RETURN\"].mean()  \n",
    "    samples_list.append({\"Predict\":av_output.item(), \"GT\":y_data.item()})\n",
    "    \n",
    "output_samples = pd.DataFrame(samples_list)\n",
    "y_pred = output_samples[\"Predict\"].values \n",
    "y_gt   = output_samples[\"GT\"].values\n",
    " \n",
    "\n",
    "y_pred = TestingData.YScaler.inverse_transform(y_pred.reshape(-1,1)) \n",
    "y_gt = TestingData.YScaler.inverse_transform(y_gt.reshape(-1,1)) \n",
    "\n",
    "from sklearn import metrics \n",
    "print(\"================== WLS =========================\") \n",
    "print(\"MAE:\",metrics.mean_absolute_error(y_pred,y_gt))\n",
    "print(\"MSE:\",metrics.mean_squared_error(y_pred,y_gt))  \n",
    "print(\"R2score:\",metrics.r2_score(y_pred,y_gt))  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EE575",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
