{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to implement OLS with python... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/GabbySuwichaya/Statistical-Learning-EE575/blob/master/Tutorial1/main.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topics covered in this exercise are: \n",
    "\n",
    "- [Examples of how to use the OLS package](#example-of-how-to-use-the-ols-package-smols-from-statsmodelsapi)\n",
    "- [Exercise *** You Implementation](#exercise-implement-ols-in-my_olsyx-using-linalgpinv)   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install numpy pandas tqdm matplotlib statsmodels ISLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import subplots \n",
    "import statsmodels.api as sm \n",
    "from ISLP import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a plotting function called `abline(ax, b, a)`  where `ax` is the matplotlib axis, `b` is the bias of linear model, `a` is the weigth of linear model... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abline(ax, b, a, *args, **kwargs):\n",
    "    \"Add a line with slope m and intercept b to ax\"\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = [a* xlim[0] + b, a * xlim[1] + b]\n",
    "    ax.plot(xlim, ylim, *args, **kwargs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of how to use the OLS package `sm.OLS` from `statsmodels.api`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we load the data to be predicted as `y` and the input data into `X`.\n",
    "\n",
    "Note that we use the entire dataset for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Boston = load_data(\"Boston\")\n",
    "Boston.columns\n",
    "\n",
    "X = pd.DataFrame({'intercept': np.ones(Boston.shape[0]),\n",
    "                  'lstat': Boston['lstat']})\n",
    "X[:4]\n",
    "\n",
    "y = Boston['medv']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now apply the data to generate the least square model from `sm.OLS` \n",
    "- Then, you will start training the model for the weights from `model.fit` \n",
    "- The learnt weights are in `results.params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, X)\n",
    "results = model.fit() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we plot the output weights using `abline(ax, b, a)`  \n",
    "Hint! You may compare what are `a` and `b` by looking at `results.params[0]` and `results.params[1]` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = Boston.plot.scatter('lstat', 'medv')\n",
    "abline(ax,\n",
    "       results.params[0],\n",
    "       results.params[1],\n",
    "       'r--',\n",
    "       linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the results from `sm.OLS` function, you can use `results.summary()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>medv</td>       <th>  R-squared:         </th> <td>   0.544</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.543</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   601.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 11 Jan 2024</td> <th>  Prob (F-statistic):</th> <td>5.08e-88</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:16:04</td>     <th>  Log-Likelihood:    </th> <td> -1641.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3287.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   504</td>      <th>  BIC:               </th> <td>   3295.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   34.5538</td> <td>    0.563</td> <td>   61.415</td> <td> 0.000</td> <td>   33.448</td> <td>   35.659</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lstat</th>     <td>   -0.9500</td> <td>    0.039</td> <td>  -24.528</td> <td> 0.000</td> <td>   -1.026</td> <td>   -0.874</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>137.043</td> <th>  Durbin-Watson:     </th> <td>   0.892</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 291.373</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.453</td>  <th>  Prob(JB):          </th> <td>5.36e-64</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.319</td>  <th>  Cond. No.          </th> <td>    29.7</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &       medv       & \\textbf{  R-squared:         } &     0.544   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.543   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     601.6   \\\\\n",
       "\\textbf{Date:}             & Thu, 11 Jan 2024 & \\textbf{  Prob (F-statistic):} &  5.08e-88   \\\\\n",
       "\\textbf{Time:}             &     15:16:04     & \\textbf{  Log-Likelihood:    } &   -1641.5   \\\\\n",
       "\\textbf{No. Observations:} &         506      & \\textbf{  AIC:               } &     3287.   \\\\\n",
       "\\textbf{Df Residuals:}     &         504      & \\textbf{  BIC:               } &     3295.   \\\\\n",
       "\\textbf{Df Model:}         &           1      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{intercept} &      34.5538  &        0.563     &    61.415  &         0.000        &       33.448    &       35.659     \\\\\n",
       "\\textbf{lstat}     &      -0.9500  &        0.039     &   -24.528  &         0.000        &       -1.026    &       -0.874     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 137.043 & \\textbf{  Durbin-Watson:     } &    0.892  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } &  291.373  \\\\\n",
       "\\textbf{Skew:}          &   1.453 & \\textbf{  Prob(JB):          } & 5.36e-64  \\\\\n",
       "\\textbf{Kurtosis:}      &   5.319 & \\textbf{  Cond. No.          } &     29.7  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   medv   R-squared:                       0.544\n",
       "Model:                            OLS   Adj. R-squared:                  0.543\n",
       "Method:                 Least Squares   F-statistic:                     601.6\n",
       "Date:                Thu, 11 Jan 2024   Prob (F-statistic):           5.08e-88\n",
       "Time:                        15:16:04   Log-Likelihood:                -1641.5\n",
       "No. Observations:                 506   AIC:                             3287.\n",
       "Df Residuals:                     504   BIC:                             3295.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     34.5538      0.563     61.415      0.000      33.448      35.659\n",
       "lstat         -0.9500      0.039    -24.528      0.000      -1.026      -0.874\n",
       "==============================================================================\n",
       "Omnibus:                      137.043   Durbin-Watson:                   0.892\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              291.373\n",
       "Skew:                           1.453   Prob(JB):                     5.36e-64\n",
       "Kurtosis:                       5.319   Cond. No.                         29.7\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Implement OLS in my_OLS(y,X) using linalg.pinv(). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement OLS in my_OLS(y,X) using linalg.pinv(). \n",
    "\n",
    "Here `my_OLS` is expected to receive the numpy arrays `y_` and `X_` \n",
    "- `y_` is an array of size n x 1 containing n measurements.  \n",
    "- `X_` is an array of size n x 2 containing nx2 input features.  \n",
    "\n",
    "How to use `linalg.pinv()` can be found here: [numpy.linalg.pinv](https://numpy.org/doc/stable/reference/generated/numpy.linalg.pinv.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "\n",
    "def my_OLS(y_, X_): \n",
    "\n",
    "    w =   np.dot(np.linalg.inv(np.dot(X_.T,X_)),np.dot(X_.T,y_)) # << Your code is here.\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we prepare `y_` and `X_` for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = y.values.reshape((len(y.values),1)) \n",
    "X_ =  np.stack([np.ones(Boston.shape[0]), Boston['lstat'].values], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the size of numpy arrays `y_` and `X_`\n",
    "e.g., y_.shape, X_.shape, or the value of X_ by printing `X_` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply my_OLS to `y_` and `X_`, you will get the estimated bias and weights in `w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = my_OLS(y_, X_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may try to print your result in the next cell and check if the lenght of `w` is the same as `results.params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w) \n",
    "# print(results.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(w) == len(results.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally here is the comparison between the linear model from `my_OLS` and `sm.OLS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    " \n",
    "ax = Boston.plot.scatter('lstat', 'medv', color=\"grey\")\n",
    "abline(ax, results.params[0], results.params[1], 'r--',  linewidth=3, label=\"OLS\")\n",
    "abline(ax, w[0], w[1], 'b:', linewidth=3, label=\"my OLS\") \n",
    "plt.legend()\n",
    "plt.grid('both')\n",
    "plt.tight_layout() \n",
    "plt.savefig(\"OLS_comparing_w.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EE575",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
